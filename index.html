<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Large Scale Detection through Adaptation by jhoffman</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">LSDA</h1>
        <p class="header"></p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/jhoffman/lsda/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/jhoffman/lsda/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/jhoffman/lsda">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/jhoffman">jhoffman</a></p>


      </header>
      <section>
   <h3>   <a name="welcome" class="anchor" href="#welcome"><span class="octicon octicon-link"></span></a>LSDA</h3>

<p>
Welcome. LSDA is a framework for large scale detection through adaptation. We combine adaptation techniques with deep convolutional models to create a fast and effective large scale detection network.
</p>
<!--
        <h3>
<a name="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>DDA: Deep Detector Adaptation</h3>

<p>
	A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. 
	Recently, deep convolutional neural networks (CNN) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. 
	Unfortunately, only a small fraction of those labels are available for the detection task. 
	It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. 
	We propose a Deep Detection Adaptation (DDA) algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors.  
Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. 
</p>

<p>
For full details on our method we are releasing an paper on arXiv. A preprint is currently available <a href="http://www.eecs.berkeley.edu/~jhoffman/papers/adaptdet-submission_7-15-14.pdf">here</a>.
</p>

-->

<h3>7.5K Detector Model</h3>
<p>
	We're releasing a 7604 category detector model for use within <a href="http://caffe.berkeleyvision.org/">Caffe</a>. The categories correspond to the 7404 leaf nodes from the <a href="http://image-net.org/">ImageNet</a> dataset, as well as 200 stonger detectors that are available with bounding box data from the <a href="www.image-net.org/challenges/LSVRC/2013/">ILSVRC2013 challenge</a> dataset. 
	
We will release the full model and a demo script once the arXiv has been approved. Stay tuned for more details...
</p>

<!--<pre><code>$ cd your_repo_root/repo_name
$ git fetch origin
$ git checkout gh-pages
</code></pre>
<-->

<h3>


<a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>This page and the corresponding software is maintained by <a href="http://www.eecs.berkeley.edu/~jhoffman/">Judy Hoffman</a> (<a href="https://github.com/jhoffman">@jhoffman</a>) and <a href="http://www.eecs.berkeley.edu/~sguada/">Sergio Guadarrama</a> (<a href="https://github.com/sguada/">@sguada</a>). The work is supported by the Berkeley vision group and BVLC. </p>


    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
